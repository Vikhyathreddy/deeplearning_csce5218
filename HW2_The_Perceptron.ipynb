{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYiZq0X2oB5t"
   },
   "source": [
    "# **CSCE 5218 / CSCE 4930 Deep Learning**\n",
    "\n",
    "# **HW1a The Perceptron** (20 pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vGVmKzgG2Ium",
    "outputId": "4cc2ca21-861a-4fba-a38c-83e3ec04bec8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 11244  100 11244    0     0  23099      0 --:--:-- --:--:-- --:--:-- 23088\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2844  100  2844    0     0  14099      0 --:--:-- --:--:-- --:--:-- 14149\n"
     ]
    }
   ],
   "source": [
    "# Get the datasets\n",
    "#!wget http://huang.eng.unt.edu/CSCE-5218/test.dat\n",
    "#!wget http://huang.eng.unt.edu/CSCE-5218/train.dat\n",
    "!curl --output train.dat http://huang.eng.unt.edu/CSCE-5218/train.dat\n",
    "!curl --output test.dat http://huang.eng.unt.edu/CSCE-5218/test.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A69DxPSc8vNs",
    "outputId": "5440e602-8ecd-44cf-d48d-2e8b00cdcc52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\t\n",
      "1\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\t0\t1\t0\n",
      "0\t0\t1\t1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t0\n",
      "0\t1\t0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
      "0\t0\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t1\t0\n",
      "0\t1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t1\t1\t0\n",
      "0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\n",
      "0\t1\t1\t0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
      "0\t0\t0\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\n",
      "0\t0\t0\t0\t0\t0\t1\t0\t1\t0\t1\t0\t1\t0\n",
      "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\n",
      "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
      "0\t0\t0\t1\t0\t0\t1\t1\t0\t1\t0\t0\t1\t0\n",
      "0\t1\t1\t1\t0\t1\t1\t1\t1\t0\t0\t0\t1\t0\n",
      "0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t0\t1\t0\n",
      "0\t1\t0\t0\t0\t1\t0\t1\t0\t1\t0\t0\t1\t0\n",
      "0\t1\t1\t0\t0\t1\t1\t1\t1\t1\t1\t0\t1\t0\n",
      "0\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
      "0\t1\t0\t0\t1\t0\t0\t1\t1\t0\t1\t1\t1\t0\n",
      "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t0\t1\t0\n"
     ]
    }
   ],
   "source": [
    "# Take a peek at the datasets\n",
    "!head train.dat\n",
    "!head test.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFXHLhnhwiBR"
   },
   "source": [
    "### Build the Perceptron Model\n",
    "\n",
    "You will need to complete some of the function definitions below.  DO NOT import any other libraries to complete this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "cXAsP_lw3QwJ"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "\n",
    "# Corpus reader, all columns but the last one are coordinates;\n",
    "#   the last column is the label\n",
    "def read_data(file_name):\n",
    "    f = open(file_name, 'r')\n",
    "\n",
    "    data = []\n",
    "    # Discard header line\n",
    "    f.readline()\n",
    "    for instance in f.readlines():\n",
    "        if not re.search('\\t', instance): continue\n",
    "        instance = list(map(int, instance.strip().split('\\t')))\n",
    "        # Add a dummy input so that w0 becomes the bias\n",
    "        instance = [-1] + instance\n",
    "        data += [instance]\n",
    "    return data\n",
    "\n",
    "\n",
    "def dot_product(array1, array2):\n",
    "    #TODO: Return dot product of array 1 and array 2\n",
    "    return sum(x_val * y_val for x_val, y_val in zip(array1, array2))\n",
    "\n",
    "def sigmoid(x):\n",
    "    #TODO: Return outpout of sigmoid function on x\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "\n",
    "# The output of the model, which for the perceptron is \n",
    "# the sigmoid function applied to the dot product of \n",
    "# the instance and the weights\n",
    "def output(weight, instance):\n",
    "    #TODO: return the output of the model \n",
    "    return sigmoid(dot_product(weight, instance))\n",
    "\n",
    "# Predict the label of an instance; this is the definition of the perceptron\n",
    "# you should output 1 if the output is >= 0.5 else output 0\n",
    "def predict(weights, instance):\n",
    "    #TODO: return the prediction of the model\n",
    "    return 1 if output(weights, instance) >= 0.5 else 0\n",
    "\n",
    "\n",
    "# Accuracy = percent of correct predictions\n",
    "def get_accuracy(weights, instances):\n",
    "    # You do not to write code like this, but get used to it\n",
    "    correct = sum([1 if predict(weights, instance) == instance[-1] else 0\n",
    "                   for instance in instances])\n",
    "    return correct * 100 / len(instances)\n",
    "\n",
    "\n",
    "# Train a perceptron with instances and hyperparameters:\n",
    "#       lr (learning rate) \n",
    "#       epochs\n",
    "# The implementation comes from the definition of the perceptron\n",
    "#\n",
    "# Training consists on fitting the parameters which are the weights\n",
    "# that's the only thing training is responsible to fit\n",
    "# (recall that w0 is the bias, and w1..wn are the weights for each coordinate)\n",
    "#\n",
    "# Hyperparameters (lr and epochs) are given to the training algorithm\n",
    "# We are updating weights in the opposite direction of the gradient of the error,\n",
    "# so with a \"decent\" lr we are guaranteed to reduce the error after each iteration.\n",
    "def train_perceptron(instances, lr, epochs):\n",
    "\n",
    "    #TODO: name this step\n",
    "    #intializing weights list\n",
    "    weights = [0] * (len(instances[0])-1)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        for instance in instances:\n",
    "            #TODO: name these steps\n",
    "            #calculating forward pass of perceptron using:1)calculate weighted sum,,3)computing error\n",
    "            in_value = dot_product(weights, instance)\n",
    "            #2) appliedsigmoid function on weighted sum\n",
    "            output = sigmoid(in_value)\n",
    "            #3)computing error from subtraction of predicted output from actual label\n",
    "            error = instance[-1] - output\n",
    "            #TODO: name these steps  updating weights depending on errors learning rate and input features of instance\n",
    "            for i in range(0, len(weights)):\n",
    "                weights[i] += lr * error * output * (1-output) * instance[i]\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adBZuMlAwiBT"
   },
   "source": [
    "## Run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "50YvUza-BYQF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n"
     ]
    }
   ],
   "source": [
    "instances_tr = read_data(\"train.dat\")\n",
    "instances_te = read_data(\"test.dat\")\n",
    "lr = 0.005\n",
    "epochs = 5\n",
    "weights = train_perceptron(instances_tr, lr, epochs)\n",
    "accuracy = get_accuracy(weights, instances_te)\n",
    "print(f\"#tr: {len(instances_tr):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
    "      f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBXkvaiQMohX"
   },
   "source": [
    "## Questions\n",
    "\n",
    "Answer the following questions. Include your implementation and the output for each question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCQ6BEk1CBlr"
   },
   "source": [
    "\n",
    "\n",
    "### Question 1\n",
    "\n",
    "In `train_perceptron(instances, lr, epochs)`, we have the follosing code:\n",
    "```\n",
    "in_value = dot_product(weights, instance)\n",
    "output = sigmoid(in_value)\n",
    "error = instance[-1] - output\n",
    "```\n",
    "\n",
    "Why don't we have the following code snippet instead?\n",
    "```\n",
    "output = predict(weights, instance)\n",
    "error = instance[-1] - output\n",
    "```\n",
    "\n",
    "#### TODO Add your answer here (text only)\n",
    "In training, we need the original output value before using the sigmoid function to calculate the error. The training process includes adding up the weighted values, using the sigmoid function, and then figuring out the error from the predicted output and the label   This helps us adjust the weights using the slope of the sigmoid function, which helps with learning.\n",
    "\n",
    "If we just used predict(weights, instance) to get a binary prediction, we wouldn't have the output we need to figure out how wrong the prediction was during training. The sigmoid function in the original code helps make the output between 0 and 1. This makes it easier to calculate the error for weight updates.\n",
    "Finding the slope of a line.\n",
    "\n",
    "The perceptron changes its weights based on how the error changes. This gradient uses the slope of the sigmoid function, which is the output multiplied by (1 -  output).\n",
    "By calculating the in_value and output separately, we can see the original output of the sigmoid function before it's turned into a prediction. This raw data is needed to calculate the slope correctly.\n",
    "Efficiency means doing things quickly and effectively.\n",
    "\n",
    "Using predict would mean we have to make an extra function call and possibly do the same calculations again within predict.\n",
    "Calculating in_value and output directly in the training loop saves time and makes the training process faster.\n",
    "Clearness or understanding.\n",
    "\n",
    "Using in_value and output directly makes the code easier to read and understand. It clearly explains how to calculate the model's result and the mistake, helping to understand the code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JU3c3m6YL2rK"
   },
   "source": [
    "### Question 2\n",
    "Train the perceptron with the following hyperparameters and calculate the accuracy with the test dataset.\n",
    "\n",
    "```\n",
    "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
    "num_epochs = [5, 10, 20, 50, 100]              # number of epochs\n",
    "lr = [0.005, 0.01, 0.05]              # learning rate\n",
    "```\n",
    "\n",
    "TODO: Write your code below and include the output at the end of each training loop (NOT AFTER EACH EPOCH)\n",
    "of your code.The output should look like the following:\n",
    "```\n",
    "# tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "# tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "# tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "[and so on for all the combinations]\n",
    "```\n",
    "You will get different results with different hyperparameters.\n",
    "\n",
    "#### TODO Add your answer here (code and output in the format above) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "G-VKJOUu2BTp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tr: 5, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 5, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 5, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 5, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 5, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 10, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 10, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 10, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 10, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 10, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 25, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 25, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 25, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 25, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 25, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 50, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 50, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 50, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 50, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 67.0\n",
      "#tr: 50, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 74.0\n",
      "#tr: 75, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 75, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 75, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 75, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 74.0\n",
      "#tr: 75, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 78.0\n",
      "#tr: 100, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 69.0\n",
      "#tr: 100, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 73.0\n",
      "#tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 77.0\n",
      "#tr: 5, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 5, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 5, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 5, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 5, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 10, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 10, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 10, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 10, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 10, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 25, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 25, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 25, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 25, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 25, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 71.0\n",
      "#tr: 50, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 50, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 50, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 50, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 74.0\n",
      "#tr: 50, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 78.0\n",
      "#tr: 75, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 75, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 75, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 70.0\n",
      "#tr: 75, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 78.0\n",
      "#tr: 75, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 80.0\n",
      "#tr: 100, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 69.0\n",
      "#tr: 100, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 70.0\n",
      "#tr: 100, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 77.0\n",
      "#tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 80.0\n",
      "#tr: 5, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 5, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 5, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 5, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 5, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 64.0\n",
      "#tr: 10, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 10, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 10, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 10, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
      "#tr: 10, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 69.0\n",
      "#tr: 25, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 25, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 67.0\n",
      "#tr: 25, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 70.0\n",
      "#tr: 25, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 74.0\n",
      "#tr: 25, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
      "#tr: 50, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
      "#tr: 50, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
      "#tr: 50, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
      "#tr: 50, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
      "#tr: 50, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 76.0\n",
      "#tr: 75, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 74.0\n",
      "#tr: 75, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
      "#tr: 75, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 79.0\n",
      "#tr: 75, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
      "#tr: 75, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
      "#tr: 100, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 69.0\n",
      "#tr: 100, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 76.0\n",
      "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n",
      "#tr: 100, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n",
      "#tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n"
     ]
    }
   ],
   "source": [
    "instances_tr = read_data(\"train.dat\")\n",
    "instances_te = read_data(\"test.dat\")\n",
    "tr_percent = [5, 10, 25, 50, 75, 100]  # percent of the training dataset to train with\n",
    "num_epochs = [5, 10, 20, 50, 100]       # number of epochs\n",
    "lr_array = [0.005, 0.01, 0.05]          # learning rate\n",
    "\n",
    "for lr in lr_array:\n",
    "    for tr_percent_value in tr_percent:\n",
    "        size = round(len(instances_tr) * tr_percent_value / 100)\n",
    "        pre_instances = instances_tr[0:size]\n",
    "\n",
    "        for epochs in num_epochs:\n",
    "            weights = train_perceptron(pre_instances, lr, epochs)\n",
    "            accuracy = get_accuracy(weights, instances_te)\n",
    "            print(f\"#tr: {tr_percent_value}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
    "                  f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFB9MtwML24O"
   },
   "source": [
    "### Question 3\n",
    "Write a couple paragraphs interpreting the results with all the combinations of hyperparameters. Drawing a plot will probably help you make a point. In particular, answer the following:\n",
    "- A. Do you need to train with all the training dataset to get the highest accuracy with the test dataset?\n",
    "- B. How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?\n",
    "   ```\n",
    "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
    "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "```\n",
    "- C. Can you get higher accuracy with additional hyperparameters (higher than `80.0`)?\n",
    "- D. Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?\n",
    "\n",
    "#### TODO: Add your answer here (code and text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38rA_Kp3wiBX"
   },
   "source": [
    "\n",
    "Analyzing the results with different sets of hyperparameters helps us learn new things. First, when we increase the amount of data used for training, the accuracy of the test results also improves. However, this getting better eventually reaches a limit. For example, when we change the learning rate to 0. 005 and train the model for 100 rounds, boosting the training data from 5% to 75% makes the accuracy much better, from 68. 0% to 780% But if we keep increasing the training data to 100%, the accuracy only goes up a little bit to 80. 0% This means that the more data you use for training, the less improvement you will see in accuracy.\n",
    "\n",
    "When we increase the number of times the model goes through the training data, it usually helps improve accuracy, but only to a certain extent. For example, when the learning speed is 0. 05 and all the data is used, the accuracy goes up from 64. 0% to 800% as the number of tries goes from 5 to 100. However, if we want to train for a longer time to get more accurate results, it will cost more in terms of computer resources.\n",
    "\n",
    "# A)No, you don't need to use all the training data to get the best results with the test data. The results show that when we use more of the training data, the accuracy gets better. But after a certain point, using even more data doesn't make much of a difference in accuracy. So, it's important to find the right balance between the amount of training data and the computer resources you have.\n",
    "\n",
    "# B) The drop in accuracy in the second try, even with more training data, could be because of a few different things. One reason could be that the model is too specific and doesn't work well with new data. More training data can make the model more likely to overfit. This is especially true if the model's complexity isn't adjusted. Also, it may be that the extra training data made the results less accurate by adding more errors or unusual data. We can make the model less complex to help fix this problem.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Results:\n",
      "  |   5  10  20  50 100\n",
      "------------------------------\n",
      " 5 | 68.0 68.0 68.0 68.0 68.0\n",
      "10 | 68.0 68.0 68.0 68.0 68.0\n",
      "25 | 68.0 68.0 68.0 68.0 64.0\n",
      "50 | 68.0 68.0 68.0 71.0 69.0\n",
      "75 | 68.0 68.0 68.0 70.0 77.0\n",
      "100 | 68.0 68.0 68.0 74.0 80.0\n"
     ]
    }
   ],
   "source": [
    "# Plot point\n",
    "tr_percent = [5, 10, 25, 50, 75, 100]\n",
    "num_epochs = [5, 10, 20, 50, 100]\n",
    "lr = [0.005, 0.01, 0.05]\n",
    "accuracies = [\n",
    "    [68.0, 68.0, 68.0, 68.0, 68.0],\n",
    "    [68.0, 68.0, 68.0, 68.0, 68.0],\n",
    "    [68.0, 68.0, 68.0, 68.0, 64.0],\n",
    "    [68.0, 68.0, 68.0, 71.0, 69.0],\n",
    "    [68.0, 68.0, 68.0, 70.0, 77.0],\n",
    "    [68.0, 68.0, 68.0, 74.0, 80.0],\n",
    "    [68.0, 68.0, 68.0, 73.0, 80.0],\n",
    "    [68.0, 68.0, 69.0, 77.0, 80.0],\n",
    "    [69.0, 69.0, 76.0, 80.0, 80.0],\n",
    "    [74.0, 77.0, 80.0, 77.0, 80.0],\n",
    "    [77.0, 80.0, 80.0, 80.0, 80.0]\n",
    "]\n",
    "\n",
    "# Plotting\n",
    "print(\"Accuracy Results:\")\n",
    "print(\"  |\", end=\"\")\n",
    "for epochs in num_epochs:\n",
    "    print(f\" {epochs:3}\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * (5 + 5 * len(num_epochs)))\n",
    "for i, percent in enumerate(tr_percent):\n",
    "    print(f\"{percent:2} |\", end=\"\")\n",
    "    for j, epochs in enumerate(num_epochs):\n",
    "        accuracy = accuracies[i][j]\n",
    "        print(f\" {accuracy:3}\", end=\"\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n",
      "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 69.0\n"
     ]
    }
   ],
   "source": [
    "instances_tr = read_data(\"train.dat\")\n",
    "instances_te = read_data(\"test.dat\")\n",
    "tr_percent = [100, 200]  # percent of the training dataset to train with\n",
    "num_epochs = [20]        # number of epochs\n",
    "lr_array = [0.050, 0.005]          # learning rate\n",
    "\n",
    "for lr, tr_percent_value in zip(lr_array, tr_percent):\n",
    "    size = round(len(instances_tr) * tr_percent_value / 100)\n",
    "    pre_instances = instances_tr[0:size]\n",
    "\n",
    "    for epochs in num_epochs:\n",
    "        weights = train_perceptron(pre_instances, lr, epochs)\n",
    "        accuracy = get_accuracy(weights, instances_te)\n",
    "        print(f\"#tr: {tr_percent_value}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
    "              f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#According to the information given, it looks like the best accuracy is 80. 0%, which is already the highest. The settings that worked best to achieve this accuracy are:\n",
    "\n",
    "#Percentage of the training data used is 75%.\n",
    "#The number of epochs is 200.\n",
    "#The speed at which we learn is 0. 005\n",
    "#Since this accuracy is already the best we've seen, we can't improve it with more hyperparameters in this dataset.\n",
    "\n",
    "#C) To get better results than 80. 0%, we might have to try different settings or models. We can change how fast we learn, use more of the training data, or use harder models. Furthermore, we can also look into methods like regularization or different ways to make things work better.\n",
    "\n",
    "#D )  It's not always worth training for longer periods while keeping other settings the same. Adding more training cycles can make the model remember the training data too well and not work well with new data. \"It's important to keep an eye on how well the model is doing on a different set of data and stop training if it starts to do worse, which means it's getting too specialized. \" Regularization techniques can also help prevent the problem of overfitting when training for a longer period of time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW2_The_Perceptron.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
